\section{Analysis}

\subsection{Logs}

To process the logs, it was necessary to get the logs out of the terminal and send them somewhere our application could use. Achieving a solution for this problem proved to be a challenging task, we thought about using the output stream to a text file, but it would be difficult to analyze an unprocessed text. It would be better if our application could get the logs formatted as JSON and even better if they were sent to a server that could later be accessed by MLog for analysis.
A Docker \cite{docker} approach seemed the best one since it was not necessary to change the Robot Shop is implementation, and the process of storing the logs would be automatized. An option we found was to use Loggly \cite{loggly} since it seemed to be easy to integrate with Docker, but we decided to try ELK because it is a common approach to this problem, so finding answers for questions would be simple, and it was also possible to use it to help with the data extraction from the logs.
In order to get the logs from Docker, it was necessary to use a driver to send the logs to ELK. The driver that was applied as a bridge for the logs to pass from Robot Shop to ELK was Gelf \cite{gelfdriver}, and it was necessary to add it to the Docker Compose \cite{dockercompose} file of Robot Shop and the configuration of Logstash.
The solution applied to solve this problem was simple but discovering it required too much effort on searching how to do it, so it was one of the most complicated tasks on this project.

\subsection{Extracted Information}

The sale logs are generated by the payment microservice, but a sale also generates logs about connectivity and other information internal to the service, even though the information about the payment is contained on a single log. To get the correct logs, we use queries on Elasticsearch, based on the field message, and searching for the total key, as we realized that only the correct logs had this pattern.
Elasticsearch returns the correct logs, but it is still necessary to get only the information about the sale since the logs carry other pieces of information such as command,  host, and level. Extracting the sale is information from the logs is one of the responsibilities of MLog is search service, and this process finds only the data that will be analyzed by MLog, such as the names of the items bought, their price, and how many where bought.
The processed data is used by the payment microservice for grouping and finding valuable intel. The information the payment service extracts consist of a list of items, each with their name, number of times it appeared on transactions, the average price of the item, the total number of items bought, and the total price of that item.